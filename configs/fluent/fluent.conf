# Modified configuration from: https://blog.ptrk.io/tweaking-an-efk-stack-on-kubernetes/
<match fluent.**>
    # this tells fluentd to not output its log on stdout
    @type null
</match>

# here we read the logs from Docker's containers and parse them
<source>
  @type tail
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type json
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>      
</source>

 <source>
   @id kube-apiserver-audit.log
   @type tail
   format json
   path /var/log/kubernetes/kube-apiserver-audit.log
   pos_file /var/log/fluentd-kube-apiserver-audit.log.pos
   time_key timestamp
   time_format %Y-%m-%dT%H:%M:%SZ
   tag kube-apiserver-audit
 </source>

# we use kubernetes metadata plugin to add metadatas to the log
<filter kubernetes.**>
    @type kubernetes_metadata
</filter>

<filter kube-apiserver-audit>
  @id kube_api_audit_normalize
  @type record_transformer
  auto_typecast false
  enable_ruby true
  <record>
    host "#{ENV['KUBERNETES_SERVICE_HOST']}"
    responseObject ${record["responseObject"].nil? ? "none": record["responseObject"].to_json}
    requestObject ${record["requestObject"].nil? ? "none": record["requestObject"].to_json}
    origin kubernetes-api-audit
  </record>
</filter>


<filter kubernetes.** systemd.** fluentd.**>
  @type elasticsearch_genid
  @id fluentd_internal_id
  hash_id_key _hash
</filter>

<filter kube-apiserver-audit>
  @type genhashvalue
  @id kube-apiserver-audit.hash
  keys auditID
  hash_type mur128    # md5/sha1/sha256/sha512
  base64_enc true
  base91_enc false
  set_key _hash
  separator _
  inc_time_as_key false
  inc_tag_as_key false
</filter>

# https://github.com/uken/fluent-plugin-elasticsearch/issues/452
 <filter **>
  @type elasticsearch_genid
  hash_id_key _hash    # storing generated hash id key (default is _hash)
</filter>

# we send the logs to Elasticsearch
<match **>
    @type elasticsearch
    include_tag_key false
    id_key _hash
    remove_keys _hash
    write_operation create
    host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
    port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
    scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
    ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
    user "#{ENV['FLUENT_ELASTICSEARCH_USER']}" # remove these lines if not needed
    password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}" # remove these lines if not needed
    reload_connections true
    logstash_prefix logstash
    logstash_format true
    <buffer>
        flush_thread_count 8
        flush_interval 5s
        chunk_limit_size 2M
        queue_limit_length 32
        retry_max_interval 30
        retry_forever true
    </buffer>
</match>
